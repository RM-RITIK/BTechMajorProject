{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T01:09:09.775124Z",
     "start_time": "2020-04-03T01:09:09.635505Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-22T08:16:43.294288Z",
     "iopub.status.busy": "2021-12-22T08:16:43.293658Z",
     "iopub.status.idle": "2021-12-22T08:16:44.845633Z",
     "shell.execute_reply": "2021-12-22T08:16:44.844526Z",
     "shell.execute_reply.started": "2021-12-22T08:16:43.294234Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLOOP is a python package that provides a convenient interface for exploring and analyzing text data. \n",
    "Behind the scene, NLOOP uses spaCy and gensim to take care of cleaning, tokenization, dependency parsing, keyword extraction and much more in one fell swoop. Here I will use it to build a keyword co-occurrence graph from the titles and abstracts of the research papers in the provided dataset. \n",
    "\n",
    "You can install NLOOP from the following address. Checkout the github repository for more examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-12-22T08:16:49.213364Z",
     "iopub.status.busy": "2021-12-22T08:16:49.213046Z",
     "iopub.status.idle": "2021-12-22T08:17:31.897143Z",
     "shell.execute_reply": "2021-12-22T08:17:31.896343Z",
     "shell.execute_reply.started": "2021-12-22T08:16:49.213334Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/syasini/NLOOP.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:17:31.899279Z",
     "iopub.status.busy": "2021-12-22T08:17:31.898963Z",
     "iopub.status.idle": "2021-12-22T08:17:36.877318Z",
     "shell.execute_reply": "2021-12-22T08:17:36.876481Z",
     "shell.execute_reply.started": "2021-12-22T08:17:31.899233Z"
    }
   },
   "outputs": [],
   "source": [
    "from nloop import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T01:09:14.7825Z",
     "start_time": "2020-04-03T01:09:13.976048Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-22T08:17:46.710245Z",
     "iopub.status.busy": "2021-12-22T08:17:46.709960Z",
     "iopub.status.idle": "2021-12-22T08:17:46.816019Z",
     "shell.execute_reply": "2021-12-22T08:17:46.814868Z",
     "shell.execute_reply.started": "2021-12-22T08:17:46.710218Z"
    }
   },
   "outputs": [],
   "source": [
    "data_fname = os.path.join(\"..\",\"input\",\"project-btech\", \"dataset.csv\")\n",
    "data = pd.read_csv(data_fname, index_col=0,)\n",
    "\n",
    " # let's look at a small sample of the data \n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T01:41:38.082268Z",
     "start_time": "2020-04-03T01:09:26.716987Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-22T08:18:01.185027Z",
     "iopub.status.busy": "2021-12-22T08:18:01.184523Z",
     "iopub.status.idle": "2021-12-22T08:18:50.849246Z",
     "shell.execute_reply": "2021-12-22T08:18:50.848523Z",
     "shell.execute_reply.started": "2021-12-22T08:18:01.184978Z"
    }
   },
   "outputs": [],
   "source": [
    "# process text with nloop\n",
    "text = Text(data[\"text\"], fast=False)\n",
    "\n",
    "# This will take a while for the entire corpus\n",
    "# use fast=True if you're only interested in clean tokens\n",
    "# and don't need dependencies, named entities, and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T01:42:18.199896Z",
     "start_time": "2020-04-03T01:41:50.820084Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-22T08:19:29.005407Z",
     "iopub.status.busy": "2021-12-22T08:19:29.005044Z",
     "iopub.status.idle": "2021-12-22T08:19:30.112669Z",
     "shell.execute_reply": "2021-12-22T08:19:30.111993Z",
     "shell.execute_reply.started": "2021-12-22T08:19:29.005358Z"
    }
   },
   "outputs": [],
   "source": [
    "# show word cloud\n",
    "text.show_wordcloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T01:42:25.284982Z",
     "start_time": "2020-04-03T01:42:24.340335Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-22T08:19:36.455332Z",
     "iopub.status.busy": "2021-12-22T08:19:36.454860Z",
     "iopub.status.idle": "2021-12-22T08:19:36.519916Z",
     "shell.execute_reply": "2021-12-22T08:19:36.518803Z",
     "shell.execute_reply.started": "2021-12-22T08:19:36.455300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show the most common token\n",
    "text_tokens = []\n",
    "for i in range (0, len(text.tokens)):\n",
    "    for j in range (0, len(text.tokens[i])):\n",
    "        text_tokens.append(text.tokens[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Deep Walk </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:23:45.300099Z",
     "iopub.status.busy": "2021-12-22T08:23:45.299697Z",
     "iopub.status.idle": "2021-12-22T08:23:45.366285Z",
     "shell.execute_reply": "2021-12-22T08:23:45.365550Z",
     "shell.execute_reply.started": "2021-12-22T08:23:45.300067Z"
    }
   },
   "outputs": [],
   "source": [
    "text = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:24:29.118426Z",
     "iopub.status.busy": "2021-12-22T08:24:29.117963Z",
     "iopub.status.idle": "2021-12-22T08:24:30.557240Z",
     "shell.execute_reply": "2021-12-22T08:24:30.556059Z",
     "shell.execute_reply.started": "2021-12-22T08:24:29.118374Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "docs = text\n",
    "count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
    "X = count_model.fit_transform(docs)\n",
    "# X[X > 0] = 1 # run this line if you don't want extra within-text cooccurence (see below)\n",
    "Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(Xc.todense()) # print out matrix in dense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:24:34.468118Z",
     "iopub.status.busy": "2021-12-22T08:24:34.467671Z",
     "iopub.status.idle": "2021-12-22T08:24:34.522707Z",
     "shell.execute_reply": "2021-12-22T08:24:34.521763Z",
     "shell.execute_reply.started": "2021-12-22T08:24:34.468087Z"
    }
   },
   "outputs": [],
   "source": [
    "words = count_model.get_feature_names()\n",
    "words[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:24:35.834702Z",
     "iopub.status.busy": "2021-12-22T08:24:35.834414Z",
     "iopub.status.idle": "2021-12-22T08:24:35.877265Z",
     "shell.execute_reply": "2021-12-22T08:24:35.876292Z",
     "shell.execute_reply.started": "2021-12-22T08:24:35.834674Z"
    }
   },
   "outputs": [],
   "source": [
    "Xc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:24:38.374025Z",
     "iopub.status.busy": "2021-12-22T08:24:38.373464Z",
     "iopub.status.idle": "2021-12-22T08:27:07.169830Z",
     "shell.execute_reply": "2021-12-22T08:27:07.168760Z",
     "shell.execute_reply.started": "2021-12-22T08:24:38.373993Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "mat = Xc.toarray()\n",
    "for i in range (0, 9929):\n",
    "    for j in range(0, 9929):\n",
    "        if mat[i][j] != 0:\n",
    "            G.add_edge(words[i], words[j], weight = mat[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:27:21.952196Z",
     "iopub.status.busy": "2021-12-22T08:27:21.951892Z",
     "iopub.status.idle": "2021-12-22T08:27:22.045249Z",
     "shell.execute_reply": "2021-12-22T08:27:22.044415Z",
     "shell.execute_reply.started": "2021-12-22T08:27:21.952166Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the node sizes using arbitrary transformation \n",
    "node_sizes= [20*G.degree[node]**2+100 for node in G.nodes]\n",
    "\n",
    "# construct the label dictionary\n",
    "labels = {i:i for i in list(G.nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:28:14.254808Z",
     "iopub.status.busy": "2021-12-22T08:28:14.254136Z",
     "iopub.status.idle": "2021-12-22T08:28:14.300326Z",
     "shell.execute_reply": "2021-12-22T08:28:14.299410Z",
     "shell.execute_reply.started": "2021-12-22T08:28:14.254757Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "def get_random_walk(graph:nx.Graph, node:int, n_steps:int = 4)->List[str]:\n",
    "   local_path = [str(node),]\n",
    "   target_node = node\n",
    "   for _ in range(n_steps):\n",
    "      neighbors = list(nx.all_neighbors(graph, target_node))\n",
    "      if len(neighbors) != 0:\n",
    "          target_node = random.choice(neighbors)\n",
    "          local_path.append(str(target_node))\n",
    "   return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:28:16.295035Z",
     "iopub.status.busy": "2021-12-22T08:28:16.294718Z",
     "iopub.status.idle": "2021-12-22T08:28:23.758099Z",
     "shell.execute_reply": "2021-12-22T08:28:23.757282Z",
     "shell.execute_reply.started": "2021-12-22T08:28:16.294985Z"
    }
   },
   "outputs": [],
   "source": [
    "walk_paths = []\n",
    "for node in G.nodes():\n",
    "   for _ in range(10):\n",
    "      walk_paths.append(get_random_walk(G, node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:28:36.702081Z",
     "iopub.status.busy": "2021-12-22T08:28:36.701773Z",
     "iopub.status.idle": "2021-12-22T08:28:36.743178Z",
     "shell.execute_reply": "2021-12-22T08:28:36.742374Z",
     "shell.execute_reply.started": "2021-12-22T08:28:36.702049Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:28:43.039288Z",
     "iopub.status.busy": "2021-12-22T08:28:43.038948Z",
     "iopub.status.idle": "2021-12-22T08:28:49.843220Z",
     "shell.execute_reply": "2021-12-22T08:28:49.842501Z",
     "shell.execute_reply.started": "2021-12-22T08:28:43.039259Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = Word2Vec(\n",
    "   window=4, sg=1, hs=0, negative=10, alpha=0.03, min_alpha=0.0001,    \n",
    "   seed=42\n",
    ")\n",
    "embedder.build_vocab(walk_paths, progress_per=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:32:14.923583Z",
     "iopub.status.busy": "2021-12-22T08:32:14.923056Z",
     "iopub.status.idle": "2021-12-22T08:32:14.986339Z",
     "shell.execute_reply": "2021-12-22T08:32:14.985351Z",
     "shell.execute_reply.started": "2021-12-22T08:32:14.923539Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder.similar_by_word('glass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:32:09.893642Z",
     "iopub.status.busy": "2021-12-22T08:32:09.893239Z",
     "iopub.status.idle": "2021-12-22T08:32:09.944469Z",
     "shell.execute_reply": "2021-12-22T08:32:09.943339Z",
     "shell.execute_reply.started": "2021-12-22T08:32:09.893606Z"
    }
   },
   "outputs": [],
   "source": [
    "len(embedder['rules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:37:10.417736Z",
     "iopub.status.busy": "2021-12-22T08:37:10.417408Z",
     "iopub.status.idle": "2021-12-22T08:37:10.537240Z",
     "shell.execute_reply": "2021-12-22T08:37:10.536607Z",
     "shell.execute_reply.started": "2021-12-22T08:37:10.417705Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "embeddings = []\n",
    "for word in words:\n",
    "    embeddings.append(embedder[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:40:48.715214Z",
     "iopub.status.busy": "2021-12-22T08:40:48.714989Z",
     "iopub.status.idle": "2021-12-22T08:40:48.758944Z",
     "shell.execute_reply": "2021-12-22T08:40:48.757932Z",
     "shell.execute_reply.started": "2021-12-22T08:40:48.715189Z"
    }
   },
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:40:05.241553Z",
     "iopub.status.busy": "2021-12-22T08:40:05.240911Z",
     "iopub.status.idle": "2021-12-22T08:40:05.292927Z",
     "shell.execute_reply": "2021-12-22T08:40:05.292075Z",
     "shell.execute_reply.started": "2021-12-22T08:40:05.241505Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['words'] = words\n",
    "df['embeddings'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-22T08:41:27.769214Z",
     "iopub.status.busy": "2021-12-22T08:41:27.768856Z",
     "iopub.status.idle": "2021-12-22T08:41:41.012326Z",
     "shell.execute_reply": "2021-12-22T08:41:41.011354Z",
     "shell.execute_reply.started": "2021-12-22T08:41:27.769164Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('DeepWalkEmbeddings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
